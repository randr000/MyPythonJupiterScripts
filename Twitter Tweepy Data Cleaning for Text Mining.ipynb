{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code I wrote to prepare Twitter data for text mining for an individual assignment for a college graduate-level course. The data was obtained using the Tweepy Python package. For academic integrity, full assignment will not be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Path to file where data to be pre-processed is saved\n",
    "input_file_path = 'tweets.csv'\n",
    "\n",
    "\n",
    "# Path to file where final pre-preocessed file is to be saved\n",
    "output_file_path = 'tweets-cleaned.csv'\n",
    "\n",
    "# Read data into a dataframe\n",
    "df = pd.read_csv(input_file_path, encoding='utf-8')\n",
    "\n",
    "# Interpret all strings with a 'b' in front as byte strings\n",
    "# and then decode using utf-8\n",
    "def decode_byte(string):\n",
    "    return ast.literal_eval(string).decode('utf8')\n",
    "\n",
    "df = df.applymap(decode_byte)\n",
    "\n",
    "\n",
    "# Replace 'None' string with python keyword None which means null\n",
    "def replace_none(string):\n",
    "    if string == 'None':\n",
    "        return None\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "df = df.applymap(replace_none)\n",
    "\n",
    "\n",
    "# Create copy of 'Text' called 'Clean_Text' to work on\n",
    "df['Clean_Text'] = df['Text']\n",
    "\n",
    "\n",
    "# Remove Twitter usernames tagged in tweet. \n",
    "# Usernames are prepended with '@'\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'(@[A-Za-z0-9_]+)', '')\n",
    "\n",
    "\n",
    "# Remove all hyperlinks\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'(https?:/?/?\\S+)',\n",
    "                                                '', flags=re.IGNORECASE)\n",
    "\n",
    "# Replace ampersands special character combinations with 'and'\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'&amp;', 'and')\n",
    "\n",
    "\n",
    "# Remove all punctuation\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'[^\\w\\s]', '')\n",
    "\n",
    "\n",
    "# Remove all newline special characters\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'(\\n)+', ' ')\n",
    "\n",
    "\n",
    "# Removed 'RT' from beginning of tweets if present\n",
    "df['Clean_Text'] = df['Clean_Text'].str.replace(r'(^RT\\s)', ' ')\n",
    "\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "df['Clean_Text'] = df['Clean_Text'].str.strip()\n",
    "\n",
    "\n",
    "# Make every letter lowercase\n",
    "df['Clean_Text'] = df['Clean_Text'].str.lower()\n",
    "\n",
    "\n",
    "# Remove all stop words\n",
    "def filter_stop_words(string):\n",
    "    stop = stopwords.words('english')\n",
    "    lst = string.split(' ')\n",
    "    lst = [x for x in lst if x not in stop]\n",
    "    return ' '.join(w for w in lst)\n",
    "\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(filter_stop_words)\n",
    "\n",
    "\n",
    "# Get rid of any extra whitespace in between words\n",
    "def only_one_space(string):\n",
    "    return re.sub(' +', ' ', string)\n",
    "\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(only_one_space)\n",
    "\n",
    "\n",
    "# Remove any text that is only one word long\n",
    "df = df[df['Clean_Text'].apply(lambda x: len(x.split(' ')) > 1)]\n",
    "\n",
    "\n",
    "# Stem all words if necessary\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(string):\n",
    "    lst = [ps.stem(w) for w in string.split(' ')]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(stem_words)\n",
    "\n",
    "\n",
    "# Output to csv preprocessed text data before tokenization\n",
    "df['Clean_Text'].to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Tokenize words\n",
    "df['Clean_Text'] = df['Clean_Text'].str.split(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
